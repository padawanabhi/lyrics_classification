{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba45a8b",
   "metadata": {},
   "source": [
    "## Steps to automate\n",
    "\n",
    "* Get Text Corpus\n",
    "* Remove punctuations\n",
    "* Remove \\n\n",
    "* Make lower case\n",
    "* tokenize\n",
    "* lemmantize (choose method)\n",
    "* Vectorize\n",
    "* Remove stop words (try)\n",
    "* TF-IDF Normalization\n",
    "* transform, train and fit model (choose model)\n",
    "* transform and predict test set\n",
    "* Return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e14db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "\n",
    "import nltk \n",
    "import contractions\n",
    "\n",
    "\n",
    "import string\n",
    "import pattern\n",
    "from pattern.en import lemma, lexeme\n",
    "\n",
    "from nltk import pos_tag\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kodaline_path = os.path.abspath('./Kodaline/') + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwa_path = os.path.abspath('./NWA/') + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "songlinks = [ kodaline_path + song for song in os.listdir('./Kodaline/')[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwa = [ nwa_path + song for song in os.listdir('./NWA/')[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for song in nwa:\n",
    "    songlinks.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = ['Kodaline']*20 + ['NWA']*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d33bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_corpus = []\n",
    "for song in songlinks:\n",
    "    with open(song, 'r') as file:\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "    lyrics_corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab8cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67633ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "songtext = \"\".join([ch for ch in lyrics_corpus[0] if ch not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ad341",
   "metadata": {},
   "outputs": [],
   "source": [
    "songtext = re.sub('\\n', ' ', songtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c796e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "songtext = songtext.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad0e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "songtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5f85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.wn.ADJ,\n",
    "                \"N\": wordnet.wn.NOUN,\n",
    "                \"V\": wordnet.wn.VERB,\n",
    "                \"R\": wordnet.wn.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0829ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemme_nltk = \" \".join([lemm.lemmatize(word, pos=get_wordnet_pos(word)) for word in songtext.split() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b930a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = TextBlob(songtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ebb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_postag(song):\n",
    "    sent = TextBlob(song)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemme_blob = lemmatize_with_postag(songtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_tokens = tokens.tokenize(songtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887268bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(song_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a514c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemme_pattern = \" \".join([lemma(word) for word in songtext.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeac79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemme_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7898c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemme_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemme_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d38241",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_words = [contractions.fix(word) for word in song_fixed.split()]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_text = ' '.join(expanded_words)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f64b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_fixed = expanded_text.replace(symbol, \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "char = [char for char in songtext if not (char.isalnum() | char.isspace())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc56e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "char1 = [char for char in lyrics_corpus[3] if not (char.isalnum() | char.isspace())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1450344",
   "metadata": {},
   "outputs": [],
   "source": [
    "songtext1 = \"\".join([ch for ch in songtext if (ch.isalnum() | ch.isspace())])\n",
    "songtext1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_new = \" \".join([contractions.fix(word) for word in songtext1.split()])\n",
    "expanded_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd2aa6",
   "metadata": {},
   "source": [
    "* Remove punctuations from song\n",
    "* Rmove new line character\n",
    "* expand contractions in song\n",
    "* lemmantize song\n",
    "* tokenize song words\n",
    "* vectorize and remove stop words\n",
    "* tfidf transformation\n",
    "* classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(corpus):\n",
    "    new_corpus = []\n",
    "    for song in corpus:\n",
    "        song = \"\".join([ch for ch in song if (ch.isalnum() | ch.isspace())])\n",
    "        new_corpus.append(song)\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = remove_punc(lyrics_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_new_line(corpus):\n",
    "    new_corpus = []\n",
    "    for song in corpus:\n",
    "        song = re.sub('\\n', ' ', song)\n",
    "        song = song.lower()\n",
    "        new_corpus.append(song)\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = remove_new_line(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contraction(corpus):\n",
    "    new_corpus = []\n",
    "    for song in corpus:\n",
    "        song = \" \".join([contractions.fix(word) for word in song.split()])\n",
    "        new_corpus.append(song)\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae50767",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = expand_contraction(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_song(corpus, method='nltk'):\n",
    "    new_corpus = []\n",
    "    try:\n",
    "        if method == 'nltk':\n",
    "            for song in corpus:\n",
    "                lemm = WordNetLemmatizer()\n",
    "                song = \" \".join([lemm.lemmatize(word, pos=get_wordnet_pos(word)) for word in song.split()])\n",
    "                new_corpus.append(song)\n",
    "        elif method == 'textblob':\n",
    "            for song in corpus:\n",
    "                song = lemmatize_with_postag(song)\n",
    "                new_corpus.append(song)\n",
    "        elif method == 'pattern':\n",
    "            for song in corpus:\n",
    "                song = \" \".join([lemma(word) for word in song.split()])\n",
    "                new_corpus.append(song)\n",
    "        return new_corpus\n",
    "        \n",
    "    except:\n",
    "        print(f\"This is not an exceptable method. User nltk, textblob or pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_corpus = lemmatize_song(corpus=new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_corpus = lemmatize_song(corpus=new_corpus, method='textblob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd5f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_corpus = lemmatize_song(corpus=new_corpus, method='pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_song = cv.fit_transform(pattern_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ad5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_song_new = tf.fit_transform(vec_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfccc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_song_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ab5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_model.fit(vec_song_new, artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543688e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_model.score(vec_song_new, artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_model.predict_proba(vec_song_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680dc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "songlinks_test = [ kodaline_path + song for song in os.listdir('./Kodaline/')[21:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwa_test = [ nwa_path + song for song in os.listdir('./NWA/')[21:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29542fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for song in nwa_test:\n",
    "    songlinks_test.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbc458",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = []\n",
    "for song in songlinks:\n",
    "    with open(song, 'r') as file:\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "    test_corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbb85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = remove_punc(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c9d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = remove_new_line(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = expand_contraction(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9354eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = lemmatize_song(test_corpus, method='pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f104f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_test = cv.transform(test_corpus)\n",
    "vec_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_test_new = tf.transform(vec_test)\n",
    "vec_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_model.predict(vec_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29874604",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_model.predict_proba(vec_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model.fit(vec_song_new, artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddc6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model.score(vec_song_new, artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d63633",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model.predict(vec_test_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
